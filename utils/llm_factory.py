"""
LLMå·¥å‚æ¨¡å—
æ”¯æŒå¤šç§LLMæä¾›å•†ï¼Œéµå¾ªä½è€¦åˆåŸåˆ™
"""
from typing import Optional
import httpx
from langchain_community.chat_models import ChatZhipuAI
from langchain_community.chat_models import ChatOpenAI
from langchain_core.language_models import BaseLanguageModel
from config.settings import settings
from utils.logger import setup_logger

logger = setup_logger("LLMFactory")


class LLMFactory:
    """LLMå·¥å‚ç±»ï¼Œç”¨äºåˆ›å»ºä¸åŒæä¾›å•†çš„LLMå®ä¾‹"""
    
    @staticmethod
    def create_llm(
        provider: Optional[str] = None,
        model_name: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> BaseLanguageModel:
        """
        åˆ›å»ºLLMå®ä¾‹
        
        Args:
            provider: LLMæä¾›å•† (zhipu/openai/iflytek)ï¼Œé»˜è®¤ä»é…ç½®è¯»å–
            model_name: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä»é…ç½®è¯»å–
            temperature: æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤ä»é…ç½®è¯»å–
            max_tokens: æœ€å¤§tokenæ•°ï¼Œé»˜è®¤ä»é…ç½®è¯»å–
        
        Returns:
            LLMå®ä¾‹
        """
        provider = provider or settings.LLM_PROVIDER
        model_name = model_name or settings.MODEL_NAME
        temperature = temperature if temperature is not None else settings.TEMPERATURE
        max_tokens = max_tokens or settings.MAX_TOKENS
        
        logger.info(f"ğŸ¤– æ­£åœ¨åˆ›å»ºLLMå®ä¾‹: provider={provider}, model={model_name}")
        
        try:
            if provider == "zhipu":
                return LLMFactory._create_zhipu(model_name, temperature, max_tokens)
            elif provider == "openai":
                return LLMFactory._create_openai(model_name, temperature, max_tokens)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}")
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºLLMå¤±è´¥: {e}")
            raise
    
    @staticmethod
    def _create_zhipu(model_name: str, temperature: float, max_tokens: int) -> ChatZhipuAI:
        """åˆ›å»ºæ™ºè°±æ¸…è¨€LLM"""
        if not settings.ZHIPU_API_KEY:
            raise ValueError("âŒ æœªé…ç½®ZHIPU_API_KEYï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶")
        
        # åˆ›å»ºè‡ªå®šä¹‰çš„ httpx clientï¼Œè®¾ç½®è¶…æ—¶æ—¶é—´ä¸º10åˆ†é’Ÿ
        timeout = httpx.Timeout(
            connect=60.0,    # è¿æ¥è¶…æ—¶ï¼š60ç§’
            read=600.0,      # è¯»å–è¶…æ—¶ï¼š10åˆ†é’Ÿ
            write=600.0,     # å†™å…¥è¶…æ—¶ï¼š10åˆ†é’Ÿ
            pool=600.0       # è¿æ¥æ± è¶…æ—¶ï¼š10åˆ†é’Ÿ
        )
        http_client = httpx.Client(timeout=timeout)
        
        logger.info(f"âœ… å·²é…ç½®HTTPè¶…æ—¶ï¼šè¿æ¥60ç§’ï¼Œè¯»å–/å†™å…¥600ç§’")
        
        return ChatZhipuAI(
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            api_key=settings.ZHIPU_API_KEY,
            http_client=http_client,  # ä¼ å…¥é…ç½®å¥½çš„ client
        )
    
    @staticmethod
    def _create_openai(model_name: str, temperature: float, max_tokens: int) -> ChatOpenAI:
        """åˆ›å»ºOpenAI LLM"""
        if not settings.OPENAI_API_KEY:
            raise ValueError("âŒ æœªé…ç½®OPENAI_API_KEYï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶")
        
        # åˆ›å»ºè‡ªå®šä¹‰çš„ httpx clientï¼Œè®¾ç½®è¶…æ—¶æ—¶é—´ä¸º10åˆ†é’Ÿ
        timeout = httpx.Timeout(
            connect=60.0,    # è¿æ¥è¶…æ—¶ï¼š60ç§’
            read=600.0,      # è¯»å–è¶…æ—¶ï¼š10åˆ†é’Ÿ
            write=600.0,     # å†™å…¥è¶…æ—¶ï¼š10åˆ†é’Ÿ
            pool=600.0       # è¿æ¥æ± è¶…æ—¶ï¼š10åˆ†é’Ÿ
        )
        http_client = httpx.Client(timeout=timeout)
        
        logger.info(f"âœ… å·²é…ç½®HTTPè¶…æ—¶ï¼šè¿æ¥60ç§’ï¼Œè¯»å–/å†™å…¥600ç§’")
        
        return ChatOpenAI(
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            api_key=settings.OPENAI_API_KEY,
            http_client=http_client,  # ä¼ å…¥é…ç½®å¥½çš„ client
        )


# ä¾¿æ·å‡½æ•°
def get_llm(**kwargs) -> BaseLanguageModel:
    """è·å–LLMå®ä¾‹çš„ä¾¿æ·å‡½æ•°"""
    return LLMFactory.create_llm(**kwargs)

