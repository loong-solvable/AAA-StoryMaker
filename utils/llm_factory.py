"""
LLMå·¥å‚æ¨¡å—
æ”¯æŒå¤šç§LLMæä¾›å•†ï¼Œéµå¾ªä½è€¦åˆåŸåˆ™
æ”¯æŒ: zhipu(æ™ºè°±æ¸…è¨€), openai, openrouter
"""
from typing import Optional
from langchain_community.chat_models import ChatOpenAI
from langchain_core.language_models import BaseLanguageModel
from config.settings import settings
from utils.logger import setup_logger
from utils.custom_zhipuai import CustomChatZhipuAI

logger = setup_logger("LLMFactory")


class LLMFactory:
    """LLMå·¥å‚ç±»ï¼Œç”¨äºåˆ›å»ºä¸åŒæä¾›å•†çš„LLMå®ä¾‹"""
    
    @staticmethod
    def create_llm(
        provider: Optional[str] = None,
        model_name: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> BaseLanguageModel:
        """
        åˆ›å»ºLLMå®ä¾‹
        
        Args:
            provider: LLMæä¾›å•† (zhipu/openai/openrouter)ï¼Œé»˜è®¤ä»é…ç½®è¯»å–
            model_name: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä»é…ç½®è¯»å–
            temperature: æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤ä»é…ç½®è¯»å–
            max_tokens: æœ€å¤§tokenæ•°ï¼Œé»˜è®¤ä»é…ç½®è¯»å–
        
        Returns:
            LLMå®ä¾‹
        """
        provider = provider or settings.LLM_PROVIDER
        
        # OpenRouterä½¿ç”¨ä¸“é—¨çš„æ¨¡å‹é…ç½®
        if provider == "openrouter":
            model_name = model_name or settings.OPENROUTER_MODEL
        else:
            model_name = model_name or settings.MODEL_NAME
            
        temperature = temperature if temperature is not None else settings.TEMPERATURE
        max_tokens = max_tokens or settings.MAX_TOKENS
        
        logger.info(f"ğŸ¤– æ­£åœ¨åˆ›å»ºLLMå®ä¾‹: provider={provider}, model={model_name}")
        
        try:
            if provider == "zhipu":
                return LLMFactory._create_zhipu(model_name, temperature, max_tokens)
            elif provider == "openai":
                return LLMFactory._create_openai(model_name, temperature, max_tokens)
            elif provider == "openrouter":
                return LLMFactory._create_openrouter(model_name, temperature, max_tokens)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}")
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºLLMå¤±è´¥: {e}")
            raise
    
    @staticmethod
    def _create_zhipu(model_name: str, temperature: float, max_tokens: Optional[int]) -> CustomChatZhipuAI:
        """åˆ›å»ºæ™ºè°±æ¸…è¨€LLMï¼ˆä½¿ç”¨è‡ªå®šä¹‰ç±»ä¿®å¤è¶…æ—¶é—®é¢˜ï¼‰"""
        if not settings.ZHIPU_API_KEY:
            raise ValueError("âŒ æœªé…ç½®ZHIPU_API_KEYï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶")
        
        # ä½¿ç”¨è‡ªå®šä¹‰çš„CustomChatZhipuAIç±»ï¼Œä¿®å¤åŸå§‹ç±»ç¡¬ç¼–ç 60ç§’è¶…æ—¶çš„é—®é¢˜
        # åŸå§‹ChatZhipuAIåœ¨_generateæ–¹æ³•ä¸­ç¡¬ç¼–ç : httpx.Client(timeout=60)
        # å¯¼è‡´æ— æ³•é…ç½®æ›´é•¿çš„è¶…æ—¶æ—¶é—´
        
        logger.info(f"â±ï¸  ä½¿ç”¨è‡ªå®šä¹‰ChatZhipuAIï¼Œé…ç½®è¶…æ—¶: 600ç§’ (10åˆ†é’Ÿ)")
        
        # æ„å»ºå‚æ•°å­—å…¸
        params = {
            "model": model_name,
            "temperature": temperature,
            "api_key": settings.ZHIPU_API_KEY,
            "request_timeout": 600.0,  # 10åˆ†é’Ÿè¶…æ—¶
        }
        if max_tokens is not None:
            params["max_tokens"] = max_tokens
        
        return CustomChatZhipuAI(**params)
    
    @staticmethod
    def _create_openai(model_name: str, temperature: float, max_tokens: Optional[int]) -> ChatOpenAI:
        """åˆ›å»ºOpenAI LLM"""
        if not settings.OPENAI_API_KEY:
            raise ValueError("âŒ æœªé…ç½®OPENAI_API_KEYï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶")
        
        # æ„å»ºå‚æ•°å­—å…¸ï¼Œå¦‚æœ max_tokens ä¸º None åˆ™ä¸ä¼ é€’è¯¥å‚æ•°
        params = {
            "model": model_name,
            "temperature": temperature,
            "api_key": settings.OPENAI_API_KEY,
        }
        if max_tokens is not None:
            params["max_tokens"] = max_tokens
        
        return ChatOpenAI(**params)
    
    @staticmethod
    def _create_openrouter(model_name: str, temperature: float, max_tokens: Optional[int]) -> ChatOpenAI:
        """
        åˆ›å»ºOpenRouter LLM
        
        OpenRouteræä¾›ç»Ÿä¸€çš„APIæ¥å…¥å¤šç§æ¨¡å‹ï¼Œå…¼å®¹OpenAI APIæ ¼å¼
        å®˜æ–¹æ–‡æ¡£: https://openrouter.ai/docs
        """
        if not settings.OPENROUTER_API_KEY:
            raise ValueError("âŒ æœªé…ç½®OPENROUTER_API_KEYï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶")
        
        logger.info(f"ğŸŒ ä½¿ç”¨OpenRouter API: {settings.OPENROUTER_BASE_URL}")
        logger.info(f"ğŸ“¦ æ¨¡å‹: {model_name}")
        
        # OpenRouterå…¼å®¹OpenAI APIï¼Œä½¿ç”¨ChatOpenAIç±»å³å¯
        # éœ€è¦è®¾ç½®base_urlæŒ‡å‘OpenRouterçš„APIç«¯ç‚¹
        params = {
            "model": model_name,
            "temperature": temperature,
            "api_key": settings.OPENROUTER_API_KEY,
            "base_url": settings.OPENROUTER_BASE_URL,
            "default_headers": {
                "HTTP-Referer": "https://github.com/AAA-StoryMaker",  # å¯é€‰ï¼šç”¨äºOpenRouterç»Ÿè®¡
                "X-Title": "AAA-StoryMaker"  # å¯é€‰ï¼šåº”ç”¨åç§°
            },
            "timeout": 600,  # 10åˆ†é’Ÿè¶…æ—¶
        }
        if max_tokens is not None:
            params["max_tokens"] = max_tokens
        
        return ChatOpenAI(**params)


# ä¾¿æ·å‡½æ•°
def get_llm(**kwargs) -> BaseLanguageModel:
    """è·å–LLMå®ä¾‹çš„ä¾¿æ·å‡½æ•°"""
    return LLMFactory.create_llm(**kwargs)

